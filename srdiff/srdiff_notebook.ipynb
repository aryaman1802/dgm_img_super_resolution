{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3342171,"sourceType":"datasetVersion","datasetId":2017696}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Single image super-resolution with diffusion probabilistic models (SRDiff)**","metadata":{}},{"cell_type":"markdown","source":"Paper: [SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models](https://arxiv.org/abs/2104.14951)\n\nHelpful Resources:\n- [SRDiff's github repo](https://github.com/LeiaLi/SRDiff/tree/main)","metadata":{}},{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-04T09:21:09.007736Z","iopub.execute_input":"2025-04-04T09:21:09.007970Z","iopub.status.idle":"2025-04-04T09:21:09.012708Z","shell.execute_reply.started":"2025-04-04T09:21:09.007946Z","shell.execute_reply":"2025-04-04T09:21:09.011609Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch import einsum\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms, models\nfrom torchinfo import summary\nfrom torch import GradScaler, autocast\n\nfrom einops import rearrange, reduce\nfrom einops.layers.torch import Rearrange\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\nfrom PIL import Image\nimport os\nimport math\nfrom functools import partial\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport pytz\nimport copy\nimport time\nimport gc\n\nprint(\"imports done!\")","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:46:35.500935Z","iopub.execute_input":"2025-04-04T09:46:35.501334Z","iopub.status.idle":"2025-04-04T09:46:35.508314Z","shell.execute_reply.started":"2025-04-04T09:46:35.501303Z","shell.execute_reply":"2025-04-04T09:46:35.507150Z"},"trusted":true},"outputs":[{"name":"stdout","text":"imports done!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def get_torch_version():\n    torch_version = torch.__version__.split(\"+\")[0]\n    torch_number = torch_version.split(\".\")[:2]\n    torch_number_float = torch_number[0] + \".\" + torch_number[1]\n    torch_number_float = float(torch_number_float)\n    return torch_number_float\n\n\ndef set_seed(seed=42):\n    \"\"\"\n    Seeds basic parameters for reproducibility of results\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        # if get_torch_version() <= 1.7:\n        #     torch.set_deterministic(True)\n        # else:\n        #     torch.use_deterministic_algorithms(True)\n    print(f\"seed {seed} set!\")\n    \n\ndef compute_accuracy(y_pred, y):\n    assert len(y_pred)==len(y), \"length of y_pred and y must be equal\"\n    acc = torch.eq(y_pred, y).sum().item()\n    acc = acc/len(y_pred)\n    return acc\n\n\ndef train_validation_split(train_dataset):\n    X_train, X_valid, y_train, y_valid = train_test_split(train_dataset.data, train_dataset.targets, \n                                                          test_size=0.2, random_state=42, shuffle=True, \n                                                          stratify=train_dataset.targets)\n    X_train = torch.tensor(X_train, dtype=torch.float64).permute(0, 3, 1, 2)\n    X_valid = torch.tensor(X_valid, dtype=torch.float64).permute(0, 3, 1, 2)\n    y_train = torch.tensor(y_train, dtype=torch.int64)\n    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n    return X_train, X_valid, y_train, y_valid\n    \n\ndef predict(model, img_path, device):\n    img = cv2.imread(img_path)\n    if img.shape[-1] == 4:\n        img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n    img2 = img.copy()\n    img = torch.tensor(img)\n    img = img.permute(1,2,0)\n    img = img.unsqueeze(dim=0)\n    img = img.to(device)\n    model.eval()\n    with torch.inference_mode():\n        logit = model(img)\n    pred_prob = torch.softmax(logit, dim=1)\n    pred_label = pred_prob.argmax(dim=1)\n    plt.imshow(img2)\n    plt.axis(\"off\")\n    plt.label(f\"Prediction: {classes[pred_label]}\\t\\tProbability: {round(pred_prob)}\")\n    plt.show()\n\n\ndef set_scheduler(scheduler, results, scheduler_on):\n    \"\"\"Makes the neccessary updates to the scheduler.\"\"\"\n    if scheduler_on == \"valid_acc\":\n        scheduler.step(results[\"valid_acc\"][-1])\n    elif scheduler_on == \"valid_loss\":\n        scheduler.step(results[\"valid_loss\"][-1])\n    elif scheduler_on == \"train_acc\":\n        scheduler.step(results[\"train_acc\"][-1])\n    elif scheduler_on == \"train_loss\":\n        scheduler.step(results[\"train_loss\"][-1])\n    else:\n        raise ValueError(\"Invalid `scheduler_on` choice.\")\n    return scheduler\n\n\ndef visualize_results(results, plot_name=None):\n    \"\"\"Plot the training and validation loss and accuracy, given the results dictionary\"\"\"\n    train_loss, train_acc = results[\"train_loss\"], results[\"train_acc\"]\n    val_loss, val_acc = results[\"valid_loss\"], results[\"valid_acc\"]\n    cls = [\"no\", \"vort\", \"sphere\"]\n    x = np.arange(len(train_loss))  # this is the number of epochs\n    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,12))\n    # ax[0,0].set_title(\"Loss\")\n    ax[0,0].set_xlabel(\"Epochs\")\n    ax[0,0].set_ylabel(\"Loss\")\n    ax[0,0].plot(x, train_loss, label=\"train_loss\", color=\"orange\")\n    ax[0,0].plot(x, val_loss, label=\"valid_loss\", color=\"blue\")\n    ax[0,0].legend()\n    # ax[0,1].set_title(\"Accuracy\")\n    ax[0,1].set_xlabel(\"Epochs\")\n    ax[0,1].set_ylabel(\"Accuracy\")\n    ax[0,1].plot(x, train_acc, label=\"train_acc\", color=\"orange\")\n    ax[0,1].plot(x, val_acc, label=\"valid_acc\", color=\"blue\")\n    ax[0,1].legend()\n    # ax[1,0].set_title(\"Train ROC AUC Plot\")\n    ax[1,0].set_xlabel(\"Epochs\")\n    ax[1,0].set_ylabel(\"Train ROC AUC Score\")\n    ax[1,0].plot(x, results[\"train_roc_auc_0\"], label=cls[0])\n    ax[1,0].plot(x, results[\"train_roc_auc_1\"], label=cls[1])\n    ax[1,0].plot(x, results[\"train_roc_auc_2\"], label=cls[2])\n    ax[1,0].legend()\n    # ax[1,1].set_title(\"Valid ROC AUC Plot\")\n    ax[1,1].set_xlabel(\"Epochs\")\n    ax[1,1].set_ylabel(\"Valid ROC AUC Score\")\n    ax[1,1].plot(x, results[\"valid_roc_auc_0\"], label=cls[0])\n    ax[1,1].plot(x, results[\"valid_roc_auc_1\"], label=cls[1])\n    ax[1,1].plot(x, results[\"valid_roc_auc_2\"], label=cls[2])\n    ax[1,1].legend()\n    if plot_name is not None:\n        plt.savefig(plot_name)\n    plt.show()\n    \n\ndef train_step(model, loss_fn, optimizer, dataloader, device, scaler=None):\n    model.train()\n    train_loss = 0\n    train_acc = 0\n    all_labels = []\n    all_preds = []\n    for X, y in dataloader:\n        X = X.to(device)\n        y = y.to(device)\n        optimizer.zero_grad()\n        if scaler is not None:           # do automatic mixed precision training\n            with autocast(device):       # mixed precision forward pass\n                logit = model(X)\n                pred_prob = torch.softmax(logit, dim=1)\n                pred_label = pred_prob.argmax(dim=1)\n                # note: first put logit and then y in the loss_fn\n                # otherwise, if you put y first and then logit, then it will raise an error\n                loss = loss_fn(logit, y)\n            scaler.scale(loss).backward()      # mixed precision backward pass\n            scaler.step(optimizer)             # updating optimizer\n            scaler.update()                    # updating weights\n        else:                     # don't do any mixed precision training\n            logit = model(X)\n            pred_prob = torch.softmax(logit, dim=1)\n            pred_label = pred_prob.argmax(dim=1)\n            loss = loss_fn(logit, y)\n            loss.backward()\n            optimizer.step()\n        all_labels.extend(y.detach().cpu().numpy())\n        all_preds.extend(pred_prob.detach().cpu().numpy())\n        train_loss += loss.item()\n        acc = compute_accuracy(pred_label, y)\n        train_acc += acc\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    return train_loss, train_acc, all_labels, all_preds\n        \n\ndef valid_step(model, loss_fn, dataloader, device):\n    model.eval()\n    valid_loss = 0\n    valid_acc = 0\n    all_labels = []\n    all_preds = []\n    with torch.inference_mode():\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            logit = model(X)\n            pred_prob = torch.softmax(logit, dim=1)\n            pred_label = pred_prob.argmax(dim=1)\n            # note: first put logit and then y in the loss_fn\n            # otherwise, if you put y first and then logit, then it will raise an error\n            loss = loss_fn(logit, y)\n            valid_loss += loss.item()\n            acc = compute_accuracy(pred_label, y)\n            valid_acc += acc\n            all_labels.extend(y.detach().cpu().numpy())\n            all_preds.extend(pred_prob.detach().cpu().numpy())\n    valid_loss = valid_loss / len(dataloader)\n    valid_acc = valid_acc / len(dataloader)\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    return valid_loss, valid_acc, all_labels, all_preds\n\n\ndef training_fn1(model, loss_fn, optimizer, train_dataloader, valid_dataloader, device, \n                 epochs, scheduler=None, scheduler_on=\"val_acc\", verbose=False, scaler=None,\n                 save_best_model=False, path=None, model_name=None, optimizer_name=None, \n                 scheduler_name=None):\n    \"\"\"\n    Does model training and validation for one fold in a k-fold cross validation setting.\n    \"\"\"\n    results = {\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"valid_loss\": [],\n        \"valid_acc\": [],\n        \"train_roc_auc_0\": [],\n        \"valid_roc_auc_0\": [],\n        \"train_roc_auc_1\": [],\n        \"valid_roc_auc_1\": [],\n        \"train_roc_auc_2\": [],\n        \"valid_roc_auc_2\": [],\n    }\n    best_valid_roc_auc = 0.0\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc, train_labels, train_preds = train_step(model, loss_fn, optimizer, \n                                                                      train_dataloader, device, scaler)\n        valid_loss, valid_acc, valid_labels, valid_preds = valid_step(model, loss_fn, valid_dataloader, \n                                                                      device)\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"valid_loss\"].append(valid_loss)\n        results[\"valid_acc\"].append(valid_acc)\n        bin_train_labels = label_binarize(train_labels, classes=[0,1,2])\n        bin_valid_labels = label_binarize(valid_labels, classes=[0,1,2])\n        mean_train_roc_auc = np.mean([results[\"train_roc_auc_0\"], results[\"train_roc_auc_1\"],\n                                          results[\"train_roc_auc_2\"]])\n        mean_valid_roc_auc = np.mean([results[\"valid_roc_auc_0\"], results[\"valid_roc_auc_1\"],\n                                          results[\"valid_roc_auc_2\"]])\n        for i in range(3):\n            try:\n                train_roc_auc = roc_auc_score(bin_train_labels[:, i], train_preds[:, i])\n                valid_roc_auc = roc_auc_score(bin_valid_labels[:, i], valid_preds[:, i])\n                results[f\"train_roc_auc_{i}\"].append(train_roc_auc)\n                results[f\"valid_roc_auc_{i}\"].append(valid_roc_auc)\n            except ValueError:\n                print(f\"Warning: AUC computation failed for class {i}\")\n        if verbose:\n            print(\n                    f\"Epoch: {epoch+1} | Train_loss: {train_loss:.5f} | \"\n                    f\"Train_acc: {train_acc:.5f} | Val_loss: {valid_loss:.5f} | \"\n                    f\"Val_acc: {valid_acc:.5f} | Train_roc_auc: {mean_train_roc_auc:.5f} | \"\n                    f\"Val_roc_auc: {mean_valid_roc_auc:.5f}\"\n                )\n        if scheduler is not None:\n            scheduler = set_scheduler(scheduler, results, scheduler_on)\n            if mean_valid_roc_auc > best_valid_roc_auc:\n                best_valid_roc_auc = mean_valid_roc_auc\n                plot_name = path + \"/\" + model_name[:-3] + \"_\" + optimizer_name[:-3] + \"_\" + scheduler_name[:-3] + \".pdf\"\n                save_model_info(path, device, model, model_name, optimizer, optimizer_name, \n                    scheduler, scheduler_name)\n        else:\n            if mean_valid_roc_auc > best_valid_roc_auc:\n                best_valid_roc_auc = mean_valid_roc_auc\n                plot_name = path + \"/\" + model_name[:-3] + \"_\" + optimizer_name[:-3] + \".pdf\"\n                save_model_info(path, device, model, model_name, optimizer, optimizer_name) \n    visualize_results(results, plot_name)\n\n\ndef training_fn2(model, loss_fn, optimizer, train_dataset, device, epochs, \n                 scheduler=None, scheduler_on=\"val_acc\", verbose=False, n_splits=5, scaler=None):\n    \"\"\"\n    Does the training and validation for all the folds in a k-fold cross validation setting.\n    \"\"\"\n    kf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n    MODELS = []\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_dataset.data, y=train_dataset.targets)):\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, \n                                      sampler=SubsetRandomSampler(train_idx))\n        valid_dataloader = DataLoader(dataset=train_dataset, batch_size=64, \n                                      sampler=SubsetRandomSampler(val_idx))\n        results = training_fn1(model, loss_fn, optimizer, train_dataloader, valid_dataloader, device, \n                                epochs, scheduler=scheduler, scheduler_on=scheduler_on, verbose=verbose)\n        train_loss = np.mean(results[\"train_loss\"])\n        valid_loss = np.mean(results[\"valid_loss\"])\n        train_acc = np.mean(results[\"train_acc\"])\n        valid_acc = np.mean(results[\"valid_acc\"])\n        print(\n                f\"Fold: {fold+1} | Train_loss: {train_loss:.5f} | \"\n                f\"Train_acc: {train_acc:.5f} | Val_loss: {valid_loss:.5f} | \"\n                f\"Val_acc: {valid_acc:.5f}\"\n            )\n        visualize_results(results)\n        MODELS.append(model)\n    return MODELS\n\n\ndef training_function(model, loss_fn, optimizer, train_dataset, device, epochs, scheduler=None, \n                      scheduler_on=\"val_acc\", verbose=False, validation_strategy=\"train test split\",\n                      n_splits=5, scaler=None):\n    \"\"\"\n    validation_strategy: choose one of the following: \n        - \"train test split\"\n        - \"k-fold cross validation\"\n    \"\"\"\n    if validation_strategy == \"train test split\":\n        X_train, X_valid, y_train, y_valid = train_validation_split(train_dataset)\n        train_dataset = CustomDataset(features=X_train, targets=y_train)\n        valid_dataset = CustomDataset(features=X_valid, targets=y_valid)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=CONFIG[\"batchsize\"], shuffle=True)\n        valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=CONFIG[\"batchsize\"], shuffle=False)\n        training_fn1(model, loss_fn, optimizer, train_dataloader, valid_dataloader, device, epochs, \n                     scheduler=scheduler, scheduler_on=scheduler_on, verbose=verbose)\n    elif validation_strategy == \"k-fold cross validation\":\n        training_fn2(model, loss_fn, optimizer, train_dataset, device, epochs, scheduler=scheduler, \n                     scheduler_on=scheduler_on, verbose=verbose, n_splits=n_splits)\n    else:\n        raise ValueError(\"Invalid validation strategy.\\nChoose either \\\"train test split\\\" \\\n        or \\\"k-fold cross validation\\\"\")\n    \n\ndef save_model_info(path: str, device, model, model_name, optimizer, optimizer_name, \n                    scheduler=None, scheduler_name=\"\"):\n    model.to(device)\n    torch.save(model.state_dict(), os.path.join(path,model_name))\n    torch.save(optimizer.state_dict(), os.path.join(path,optimizer_name))\n    if scheduler is not None:\n        torch.save(scheduler.state_dict(), os.path.join(path,scheduler_name))    \n    print(\"Model info saved!\")\n    \n    \ndef load_model_info(PATH, device, model, model_name, optimizer, optimizer_name, \n                    scheduler=None, scheduler_name=\"\"):\n    model.load_state_dict(torch.load(os.path.join(path,model_name)))\n    model.to(device)\n    optimizer.load_state_dict(torch.load(os.path.join(path,optimizer_name)))\n    if scheduler is not None:\n        scheduler.load_state_dict(torch.load(os.path.join(path,scheduler_name)))\n    print(\"Model info loaded!\")\n    \n    \ndef get_current_time():\n    \"\"\"Returns the current time in Toronto.\"\"\"\n    now = datetime.now(pytz.timezone('Canada/Eastern'))\n    current_time = now.strftime(\"%d_%m_%Y__%H_%M_%S\")\n    return current_time\n\n\nprint(\"Utility functions created!\")","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:21:17.084873Z","iopub.execute_input":"2025-04-04T09:21:17.085437Z","iopub.status.idle":"2025-04-04T09:21:17.130720Z","shell.execute_reply.started":"2025-04-04T09:21:17.085396Z","shell.execute_reply":"2025-04-04T09:21:17.129651Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Utility functions created!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"get_torch_version()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:21:17.132182Z","iopub.execute_input":"2025-04-04T09:21:17.132576Z","iopub.status.idle":"2025-04-04T09:21:17.162869Z","shell.execute_reply.started":"2025-04-04T09:21:17.132544Z","shell.execute_reply":"2025-04-04T09:21:17.161832Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"2.5"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:21:17.163795Z","iopub.execute_input":"2025-04-04T09:21:17.164050Z","iopub.status.idle":"2025-04-04T09:21:17.189575Z","shell.execute_reply.started":"2025-04-04T09:21:17.164029Z","shell.execute_reply":"2025-04-04T09:21:17.188430Z"},"trusted":true},"outputs":[{"name":"stdout","text":"seed 42 set!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:21:17.190693Z","iopub.execute_input":"2025-04-04T09:21:17.191014Z","iopub.status.idle":"2025-04-04T09:21:17.196869Z","shell.execute_reply.started":"2025-04-04T09:21:17.190989Z","shell.execute_reply":"2025-04-04T09:21:17.195943Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'cpu'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"path = \"../input/\"\ntrain_path = path + \"DIV2K_train_HR/DIV2K_train_HR/\"\nvalid_path = path+\"DIV2K_valid_HR/DIV2K_valid_HR/\"\nprint(\"No. of images in the training dataset:\", len(os.listdir(train_path)))\nprint(\"No. of images in the validation dataset:\", len(os.listdir(valid_path)))","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:46:11.578084Z","iopub.execute_input":"2025-04-04T09:46:11.578479Z","iopub.status.idle":"2025-04-04T09:46:11.640069Z","shell.execute_reply.started":"2025-04-04T09:46:11.578450Z","shell.execute_reply":"2025-04-04T09:46:11.638897Z"},"trusted":true},"outputs":[{"name":"stdout","text":"No. of images in the training dataset: 800\nNo. of images in the validation dataset: 100\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def fn():\n    train_img_name = random.choice(os.listdir(train_path))\n    valid_img_name = random.choice(os.listdir(valid_path))\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,12))\n    ax[0].imshow(plt.imread(train_path+train_img_name))\n    ax[0].axis(\"off\")\n    ax[0].set_title(\"Train Image\")\n    ax[1].imshow(plt.imread(valid_path+valid_img_name))\n    ax[1].axis(\"off\")\n    ax[1].set_title(\"Validation Image\")\n    plt.show()\n\nfn()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:21:17.271049Z","iopub.status.idle":"2025-04-04T09:21:17.271427Z","shell.execute_reply":"2025-04-04T09:21:17.271295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Conditional Network (Feature Extractor)**","metadata":{}},{"cell_type":"code","source":"# Residual Dense Block (RDB)\nclass ResidualDenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_channels=32, num_layers=5):\n        super(ResidualDenseBlock, self).__init__()\n        self.num_layers = num_layers\n        self.growth_channels = growth_channels\n        \n        # Create convolutional layers for dense connections\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.layers.append(\n                nn.Conv2d(in_channels + i * growth_channels, growth_channels, kernel_size=3, padding=1)\n            )\n        # Local feature fusion layer to combine features from all layers\n        self.lff = nn.Conv2d(in_channels + num_layers * growth_channels, in_channels, kernel_size=1)\n    \n    def forward(self, x):\n        features = [x]\n        for layer in self.layers:\n            # Concatenate previous features along the channel dimension\n            concat_features = torch.cat(features, dim=1)\n            out = F.relu(layer(concat_features))\n            features.append(out)\n        # Fuse all concatenated features\n        concat_features = torch.cat(features, dim=1)\n        fused = self.lff(concat_features)\n        # Apply residual connection with scaling to stabilize training\n        return fused * 0.2 + x\n\n# Conditional Network using multiple Residual Dense Blocks\nclass ConditionalNet(nn.Module):\n    def __init__(self, in_channels=3, num_features=64, num_blocks=5):\n        super(ConditionalNet, self).__init__()\n        # Initial convolution layer to extract basic features\n        self.conv_first = nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1)\n        \n        # Sequence of ResidualDenseBlocks\n        self.rdb_blocks = nn.Sequential(\n            *[ResidualDenseBlock(num_features) for _ in range(num_blocks)]\n        )\n        \n        # Final convolution to produce the conditioned feature map\n        self.conv_last = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n    \n    def forward(self, x):\n        out = self.conv_first(x)\n        out = self.rdb_blocks(out)\n        out = self.conv_last(out)\n        return out\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a dummy low-resolution image tensor (batch size=1, channels=3, 64x64 image)\n    lr_image = torch.randn(1, 3, 64, 64)\n    model = ConditionalNet()\n    features = model(lr_image)\n    print(\"Extracted features shape:\", features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:32:58.956138Z","iopub.execute_input":"2025-04-04T09:32:58.956542Z","iopub.status.idle":"2025-04-04T09:32:59.146896Z","shell.execute_reply.started":"2025-04-04T09:32:58.956514Z","shell.execute_reply":"2025-04-04T09:32:59.145845Z"}},"outputs":[{"name":"stdout","text":"Extracted features shape: torch.Size([1, 64, 64, 64])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### **Diffusion Model**","metadata":{}},{"cell_type":"code","source":"def get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    Create sinusoidal embeddings for the given timesteps.\n    timesteps: a tensor of shape (N,)\n    embedding_dim: dimension of the embedding\n    \"\"\"\n    half_dim = embedding_dim // 2\n    emb_factor = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb_factor)\n    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = F.pad(emb, (0, 1))\n    return emb\n\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim):\n        super(DownBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n        self.downsample = nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n    \n    def forward(self, x, t_emb):\n        h = self.conv(x)\n        # Incorporate time embedding: broadcast to spatial dimensions\n        time_emb = self.time_mlp(t_emb).unsqueeze(-1).unsqueeze(-1)\n        h = h + time_emb\n        h_down = self.downsample(h)\n        return h_down, h  # Return downsampled feature and skip connection\n\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim):\n        super(UpBlock, self).__init__()\n        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n        self.conv = nn.Sequential(\n            nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n    \n    def forward(self, x, skip, t_emb):\n        h = self.upconv(x)\n        # Concatenate skip connection from down path\n        h = torch.cat([h, skip], dim=1)\n        h = self.conv(h)\n        time_emb = self.time_mlp(t_emb).unsqueeze(-1).unsqueeze(-1)\n        h = h + time_emb\n        return h\n\nclass UNetDiffusion(nn.Module):\n    def __init__(self, in_channels=3, base_channels=64, time_emb_dim=128):\n        super(UNetDiffusion, self).__init__()\n        self.time_emb_dim = time_emb_dim\n        \n        # Time embedding MLP\n        self.time_mlp = nn.Sequential(\n            nn.Linear(time_emb_dim, time_emb_dim),\n            nn.ReLU(),\n            nn.Linear(time_emb_dim, time_emb_dim)\n        )\n        \n        # Initial convolution to process the noisy input\n        self.init_conv = nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1)\n        \n        # Downsampling path\n        self.down1 = DownBlock(base_channels, base_channels*2, time_emb_dim)\n        self.down2 = DownBlock(base_channels*2, base_channels*4, time_emb_dim)\n        \n        # Bottleneck layer\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(base_channels*4, base_channels*4, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        \n        # Upsampling path\n        self.up1 = UpBlock(base_channels*4, base_channels*2, time_emb_dim)\n        self.up2 = UpBlock(base_channels*2, base_channels, time_emb_dim)\n        \n        # Final convolution to produce the denoised output image\n        self.final_conv = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)\n    \n    def forward(self, x, t, cond_features=None):\n        \"\"\"\n        x: Noisy image tensor of shape (B, C, H, W)\n        t: Tensor containing the time step (e.g., shape (B,))\n        cond_features: Optional conditional features from the Conditional Network\n        \"\"\"\n        # Generate and process time embeddings\n        t_emb = get_timestep_embedding(t, self.time_emb_dim)\n        t_emb = self.time_mlp(t_emb)\n        \n        # Initial feature extraction\n        h0 = self.init_conv(x)\n        \n        # Downsampling with skip connections\n        h1, skip1 = self.down1(h0, t_emb)\n        h2, skip2 = self.down2(h1, t_emb)\n        \n        # Bottleneck processing\n        h_mid = self.bottleneck(h2)\n        \n        # Upsampling and merging with skip connections\n        h_up1 = self.up1(h_mid, skip2, t_emb)\n        h_up2 = self.up2(h_up1, skip1, t_emb)\n        \n        # Optionally add conditional features from the LR image\n        if cond_features is not None:\n            h_up2 = h_up2 + cond_features\n        \n        # Final convolution to produce the output image\n        out = self.final_conv(h_up2)\n        return out\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a dummy noisy image tensor (batch size=1, channels=3, 64x64 image)\n    noisy_img = torch.randn(1, 3, 64, 64)\n    # Create a dummy time step tensor (e.g., a single diffusion step)\n    t = torch.tensor([10])\n    \n    model = UNetDiffusion()\n    denoised = model(noisy_img, t)\n    print(\"Denoised image shape:\", denoised.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:35:50.680460Z","iopub.execute_input":"2025-04-04T09:35:50.680801Z","iopub.status.idle":"2025-04-04T09:35:50.685979Z","shell.execute_reply.started":"2025-04-04T09:35:50.680765Z","shell.execute_reply":"2025-04-04T09:35:50.684980Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### **Noise Schedule and Forward Diffusion Process**","metadata":{}},{"cell_type":"code","source":"class DiffusionSchedule:\n    def __init__(self, num_timesteps, beta_start=1e-4, beta_end=0.02):\n        \"\"\"\n        Initializes the diffusion schedule.\n        num_timesteps: Total number of diffusion steps.\n        beta_start, beta_end: Defines the linear schedule for beta.\n        \"\"\"\n        self.num_timesteps = num_timesteps\n        # Create a linear schedule for beta\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        # Calculate alphas: αₜ = 1 - βₜ\n        self.alphas = 1.0 - self.betas\n        # Compute cumulative product: \\bar{α}_t = ∏_{s=1}^t α_s\n        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n\n    def get_alpha_bar(self, t):\n        \"\"\"\n        Retrieve \\bar{α}_t for a given time step t.\n        t: A tensor of time steps (shape: [batch_size])\n        Returns: Tensor of corresponding alpha_bar values (shape: [batch_size, 1, 1, 1])\n        \"\"\"\n        # Indexing into alpha_bars for each t and reshape to broadcast over image dimensions\n        alpha_bar = self.alpha_bars[t].view(-1, 1, 1, 1)\n        return alpha_bar\n\ndef forward_diffusion_sample(x0, t, schedule):\n    \"\"\"\n    Perform the forward diffusion process.\n    x0: Original clean image tensor of shape (B, C, H, W)\n    t: Tensor containing the diffusion time steps (shape: [B])\n    schedule: Instance of DiffusionSchedule\n    Returns: Noisy image x_t and the sampled noise epsilon.\n    \"\"\"\n    # Get corresponding alpha_bar for each t\n    alpha_bar = schedule.get_alpha_bar(t)\n    # Sample random Gaussian noise\n    noise = torch.randn_like(x0)\n    # Compute noisy image using the forward process equation\n    xt = torch.sqrt(alpha_bar) * x0 + torch.sqrt(1 - alpha_bar) * noise\n    return xt, noise\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define parameters for the noise schedule\n    num_timesteps = 1000\n    schedule = DiffusionSchedule(num_timesteps=num_timesteps)\n    \n    # Create a dummy high-resolution image tensor (batch size=1, channels=3, 64x64 image)\n    hr_image = torch.randn(1, 3, 64, 64)\n    # Create a tensor for time step, e.g., t=10 for the current batch (ensure type is long for indexing)\n    t = torch.tensor([10], dtype=torch.long)\n    \n    # Generate the noisy image\n    noisy_image, noise = forward_diffusion_sample(hr_image, t, schedule)\n    print(\"Noisy image shape:\", noisy_image.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:38:01.516066Z","iopub.execute_input":"2025-04-04T09:38:01.516476Z","iopub.status.idle":"2025-04-04T09:38:01.543632Z","shell.execute_reply.started":"2025-04-04T09:38:01.516439Z","shell.execute_reply":"2025-04-04T09:38:01.542527Z"}},"outputs":[{"name":"stdout","text":"Noisy image shape: torch.Size([1, 3, 64, 64])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### **Residual Prediction Module**","metadata":{}},{"cell_type":"code","source":"# Define a simple Residual Block used in the Residual Prediction Module.\nclass ResidualBlock(nn.Module):\n    def __init__(self, num_features):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n    \n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        return out + identity\n\n# Residual Prediction Module.\nclass ResidualPredictionNet(nn.Module):\n    def __init__(self, in_channels=3, num_features=64, num_residual_blocks=5, scale_factor=4):\n        \"\"\"\n        in_channels: Number of channels in the input image (e.g., 3 for RGB).\n        num_features: Number of feature maps used in intermediate layers.\n        num_residual_blocks: How many residual blocks to use.\n        scale_factor: Upscaling factor to reach high-resolution.\n        \"\"\"\n        super(ResidualPredictionNet, self).__init__()\n        # Upsampling layer using bilinear interpolation.\n        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n        # An initial convolution layer to process the upsampled image.\n        self.entry_conv = nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1)\n        # Residual blocks to learn the missing details.\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(num_features) for _ in range(num_residual_blocks)]\n        )\n        # A final convolution layer to predict the residual image.\n        self.exit_conv = nn.Conv2d(num_features, in_channels, kernel_size=3, padding=1)\n    \n    def forward(self, lr_image):\n        # Upsample the low-resolution image to the desired high-resolution size.\n        upsampled = self.upsample(lr_image)\n        x = self.entry_conv(upsampled)\n        x = self.residual_blocks(x)\n        residual = self.exit_conv(x)\n        # The final prediction is the upsampled image plus the predicted residual.\n        hr_pred = upsampled + residual\n        return hr_pred\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a dummy low-resolution image tensor\n    # Let's say the LR image has shape (batch_size=1, channels=3, height=16, width=16)\n    # With a scale factor of 4, the HR image will have size 64x64.\n    lr_image = torch.randn(1, 3, 16, 16)\n    model = ResidualPredictionNet()\n    hr_image = model(lr_image)\n    print(\"Predicted HR image shape:\", hr_image.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:39:22.194050Z","iopub.execute_input":"2025-04-04T09:39:22.194454Z","iopub.status.idle":"2025-04-04T09:39:22.254317Z","shell.execute_reply.started":"2025-04-04T09:39:22.194418Z","shell.execute_reply":"2025-04-04T09:39:22.253293Z"}},"outputs":[{"name":"stdout","text":"Predicted HR image shape: torch.Size([1, 3, 64, 64])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### **Data Preprocessing and Dataset Pipeline**","metadata":{}},{"cell_type":"code","source":"class DIV2KDataset(Dataset):\n    def __init__(self, hr_dir, scale_factor=4, transform=None):\n        \"\"\"\n        hr_dir: Path to the directory containing high resolution images.\n        scale_factor: Factor by which to downscale the HR image to generate the LR image.\n        transform: Optional additional transformations to be applied on the HR image.\n        \"\"\"\n        self.hr_dir = hr_dir\n        # Collect all image paths with common image extensions.\n        self.hr_image_paths = [\n            os.path.join(hr_dir, f) for f in os.listdir(hr_dir)\n            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n        ]\n        self.scale_factor = scale_factor\n        self.transform = transform\n        self.to_tensor = transforms.ToTensor()\n    \n    def __len__(self):\n        return len(self.hr_image_paths)\n    \n    def __getitem__(self, idx):\n        # Load HR image and ensure it's in RGB format.\n        hr_path = self.hr_image_paths[idx]\n        hr_image = Image.open(hr_path).convert(\"RGB\")\n        \n        # Optionally apply additional transformations.\n        if self.transform:\n            hr_image = self.transform(hr_image)\n        \n        # Convert the HR image to a tensor.\n        hr_tensor = self.to_tensor(hr_image)\n        \n        # Generate the corresponding LR image by downsampling using bicubic interpolation.\n        w, h = hr_image.size\n        lr_w, lr_h = w // self.scale_factor, h // self.scale_factor\n        lr_image = hr_image.resize((lr_w, lr_h), Image.BICUBIC)\n        lr_tensor = self.to_tensor(lr_image)\n        \n        return lr_tensor, hr_tensor\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the directory containing DIV2K HR images.\n    hr_directory = train_path\n    # Initialize the dataset with a 4x downscaling factor.\n    dataset = DIV2KDataset(hr_directory, scale_factor=4)\n    # Create a DataLoader for batching and shuffling.\n    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n    \n    # Iterate over the dataset.\n    for lr_batch, hr_batch in dataloader:\n        print(\"Low-resolution batch shape:\", lr_batch.shape)\n        print(\"High-resolution batch shape:\", hr_batch.shape)\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:46:43.090845Z","iopub.execute_input":"2025-04-04T09:46:43.091178Z","iopub.status.idle":"2025-04-04T09:46:44.094820Z","shell.execute_reply.started":"2025-04-04T09:46:43.091151Z","shell.execute_reply":"2025-04-04T09:46:44.093461Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-80b4bbd0410b>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Iterate over the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Low-resolution batch shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"High-resolution batch shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 339, 510] at entry 0 and [3, 261, 510] at entry 2"],"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [3, 339, 510] at entry 0 and [3, 261, 510] at entry 2","output_type":"error"}],"execution_count":21},{"cell_type":"markdown","source":"### **Training and Evaluation Pipeline**","metadata":{}},{"cell_type":"code","source":"# Assume the following components have been defined:\n# - ConditionalNet (from Component 1)\n# - UNetDiffusion (from Component 2)\n# - DiffusionSchedule and forward_diffusion_sample (from Component 3)\n# - DIV2KDataset (from Component 4)\n\ndef train(diffusion_model, cond_net, diffusion_schedule, dataloader, num_epochs=10, device='cuda'):\n    \"\"\"\n    Training loop for the diffusion model.\n    \n    Args:\n      diffusion_model: The U-Net diffusion model.\n      cond_net: The conditional network (pre-trained and fixed).\n      diffusion_schedule: The noise schedule instance.\n      dataloader: DataLoader providing (LR, HR) image pairs.\n      num_epochs: Number of training epochs.\n      device: 'cuda' or 'cpu'.\n    \"\"\"\n    diffusion_model.train()\n    # Set conditional network to evaluation mode if pre-trained\n    cond_net.eval()  \n    optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-4)\n    mse_loss = torch.nn.MSELoss()\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for batch_idx, (lr, hr) in enumerate(dataloader):\n            lr = lr.to(device)\n            hr = hr.to(device)\n            \n            # Extract conditional features from LR images\n            with torch.no_grad():\n                cond_features = cond_net(lr)\n            \n            # Sample a random diffusion time step for each image in the batch\n            batch_size = hr.size(0)\n            t = torch.randint(0, diffusion_schedule.num_timesteps, (batch_size,), device=device).long()\n            \n            # Generate a noisy version of the HR image using the forward diffusion process\n            xt, noise = forward_diffusion_sample(hr, t, diffusion_schedule)\n            xt = xt.to(device)\n            noise = noise.to(device)\n            \n            # Predict the noise from the diffusion model\n            pred_noise = diffusion_model(xt, t, cond_features)\n            \n            # Compute loss: how well did the model predict the added noise?\n            loss = mse_loss(pred_noise, noise)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            \n            if batch_idx % 10 == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n        \n        avg_loss = epoch_loss / len(dataloader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}\")\n\ndef evaluate(diffusion_model, cond_net, diffusion_schedule, lr_image, num_steps=1000, device='cuda'):\n    \"\"\"\n    A sketch of the evaluation/inference procedure.\n    \n    Args:\n      diffusion_model: The trained diffusion model.\n      cond_net: The conditional network.\n      diffusion_schedule: The noise schedule.\n      lr_image: A low-resolution image tensor.\n      num_steps: Total diffusion steps (should match training schedule).\n      device: 'cuda' or 'cpu'.\n    \n    Returns:\n      hr_pred: The generated high-resolution image.\n    \"\"\"\n    diffusion_model.eval()\n    cond_net.eval()\n    \n    with torch.no_grad():\n        # Extract conditional features from the LR image\n        cond_features = cond_net(lr_image.to(device))\n        \n        # Start from pure noise\n        hr_pred = torch.randn(lr_image.size(0), 3, lr_image.size(2) * 4, lr_image.size(3) * 4, device=device)\n        \n        # Here we would iteratively apply the reverse diffusion process.\n        # For brevity, this is a simplified loop.\n        for t in reversed(range(num_steps)):\n            t_tensor = torch.full((lr_image.size(0),), t, device=device, dtype=torch.long)\n            # One step of the reverse diffusion process:\n            hr_pred = diffusion_model(hr_pred, t_tensor, cond_features)\n            # Additional noise correction and scaling would be applied here in a full implementation.\n    \n    return hr_pred\n\n# Example usage:\nif __name__ == \"__main__\":\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Initialize the models\n    cond_net = ConditionalNet().to(device)\n    diffusion_model = UNetDiffusion().to(device)\n    # Assume the conditional network is pre-trained; here, we keep it fixed.\n    \n    # Create the diffusion schedule (e.g., 1000 timesteps)\n    diffusion_schedule = DiffusionSchedule(num_timesteps=1000)\n    \n    # Initialize the DIV2K dataset and DataLoader\n    dataset = DIV2KDataset(hr_dir=\"data/raw/DIV2K\", scale_factor=4)\n    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n    \n    # Train the model\n    train(diffusion_model, cond_net, diffusion_schedule, dataloader, num_epochs=5, device=device)\n    \n    # For evaluation, assume we take one low-resolution image from the dataset\n    lr_sample, _ = dataset[0]\n    lr_sample = lr_sample.unsqueeze(0)  # Add batch dimension\n    hr_generated = evaluate(diffusion_model, cond_net, diffusion_schedule, lr_sample, num_steps=1000, device=device)\n    print(\"Generated HR image shape:\", hr_generated.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Loss Functions and Optimization Strategy**","metadata":{}},{"cell_type":"code","source":"# Define a custom loss module for the diffusion model.\nclass DiffusionLoss(nn.Module):\n    def __init__(self):\n        super(DiffusionLoss, self).__init__()\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, pred_noise, true_noise):\n        \"\"\"\n        Computes the mean squared error between the predicted noise and the actual noise.\n        \n        Args:\n          pred_noise: The noise predicted by the diffusion model.\n          true_noise: The actual noise that was added during the forward process.\n        \n        Returns:\n          loss: A scalar loss value.\n        \"\"\"\n        loss = self.mse_loss(pred_noise, true_noise)\n        return loss\n\n# Example setup of the optimizer and usage of the loss in a training step.\ndef training_step(diffusion_model, cond_net, diffusion_schedule, lr_batch, hr_batch, device):\n    # Set models to proper modes.\n    diffusion_model.train()\n    cond_net.eval()  # Assuming the conditional network is pre-trained.\n    \n    # Extract conditional features.\n    with torch.no_grad():\n        cond_features = cond_net(lr_batch.to(device))\n    \n    # Sample random time steps for each image in the batch.\n    batch_size = hr_batch.size(0)\n    t = torch.randint(0, diffusion_schedule.num_timesteps, (batch_size,), device=device).long()\n    \n    # Generate the noisy image and corresponding noise using the forward diffusion process.\n    xt, noise = forward_diffusion_sample(hr_batch.to(device), t, diffusion_schedule)\n    \n    # Predict the noise using the diffusion model.\n    pred_noise = diffusion_model(xt, t, cond_features)\n    \n    # Compute the loss.\n    loss_fn = DiffusionLoss().to(device)\n    loss = loss_fn(pred_noise, noise)\n    \n    # Set up the optimizer (here we use Adam).\n    optimizer = optim.Adam(diffusion_model.parameters(), lr=1e-4)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    return loss.item()\n\n# Example usage (assuming required components are already defined and instantiated):\nif __name__ == \"__main__\":\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Dummy instantiation of models (in practice, these are your defined models).\n    cond_net = ConditionalNet().to(device)\n    diffusion_model = UNetDiffusion().to(device)\n    \n    # Create a diffusion schedule (e.g., 1000 timesteps).\n    diffusion_schedule = DiffusionSchedule(num_timesteps=1000)\n    \n    # Create dummy batches for low-resolution and high-resolution images.\n    # Here, we assume lr_batch shape: (batch_size, 3, H, W) and hr_batch shape accordingly.\n    lr_batch = torch.randn(4, 3, 64, 64)  # e.g., batch size=4, LR images 64x64.\n    hr_batch = torch.randn(4, 3, 256, 256)  # HR images corresponding to 4x upscaling.\n    \n    # Perform one training step.\n    loss_val = training_step(diffusion_model, cond_net, diffusion_schedule, lr_batch, hr_batch, device)\n    print(\"Training step loss:\", loss_val)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}